\documentclass[a4paper, 11pt]{article}
\hyphenpenalty=8000
\textwidth=125mm
\textheight=185mm

\usepackage{graphicx}
%this package is flexible for image insertion
%
\usepackage{alltt}
%this package is suitable for the description of algorithms and computer programs
%
\usepackage{amsmath}
%this package draws mathematical symbols smoothly
%
\usepackage[hidelinks, pdftex]{hyperref}
%this package produces hypertext links in the document

\pagenumbering{arabic}
\setcounter{page}{1}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\doi}[1]{\href{https://doi.org/#1}{\texttt{https://doi.org/#1}}}

\begin{document}
\begin{sloppypar}
\begin{center}
Applied Artificial Intelligence Institute (A2I2)\\
Deakin University\\
\today\\[24pt]
\LARGE
\textbf{Problem Overview}\\[6pt]
\small
\textbf {Long Van Tran}\\[6pt]
s224930257@deakin.edu.au\\[6pt]
% Received: date\quad/\quad
% Revised: date\quad/\quad
% Publised online: data
\end{center}

% \begin{abstract}
% A short abstract describing the research done, methodology, and achieved results is to be presented. The abstract should contain approximately 100 words. The volume of the article is up to 20 pages in NA journal style. The journal recognizes review articles as special ones. These articles (if any) will occupy the starting positions in the journal and may contain more than 20 pages. Text in article have to follow a few simple guidelines: complex mathematical expressions have to be justified (like in the excerpt below), algorithms have to be presented in the style of \texttt{alltt}, \textit{postscript specials} have to be absent in file format presenting images. The enumeration of references within any article has to be organized in alphabetic order (see the sample below). References have to be distinguished between journal articles, collective works, and books and presented in BibTeX\ \texttt{NAplain} style (see the corresponding section below and the file \texttt{sample.bib}). \vskip 2mm

% \textbf{Keywords:} a few keywords (2--5) essential to the content of the article.

% \end{abstract}


\section{Problem Setting}\label{s:1}
In a conventional setting, we have a time-series dataset $\{\mathbf{X}^{(n)}\}_{n=1}^N$,
\[
\mathbf{X}^{(n)} 
= 
\bigl[\mathbf{X}_{t_{1}}^{(n)}, \mathbf{X}_{t_{2}}^{(n)}, \ldots, \mathbf{X}_{t_{k}}^{(n)}\bigr],
\]
where $\mathbf{X}_{t_{i}}^{(n)}$ can be vectors ($\mathbf{X}_{t_{i}}^{(n)} \in \mathbf{R}^d$) or matrices
($\mathbf{X}_{t_{i}}^{(n)} \in \mathbf{R}^{t \times d}$), depending on the unit of time being considered
here is a single time step or a segment of $t$ steps. Here we consider the latter matrix case.
Each of these trajectories describe patient records across \(k\) time points/segments
(e.g., patient measurements throughout \(k\) hours).

We assume that the data is governed by a time-homogeneous linear additive noise stochastic
differential equation (SDE), which has the form:
\[
\mathrm{d}\mathbf{X}_t 
= 
\mathbf{A}\mathbf{X}_t\mathrm{d}t 
\;+\; 
\mathbf{G}\mathrm{d}\mathbf{W}_t,
\]
where $\mathbf{A} \in \mathbf{R}^{d \times d}$ and $\mathbf{G} \in \mathbf{R}^{d \times m}$ are the 
unknown drift-diffusion parameters. $\mathbf{W}$ is an $m$-dimensional Brownian motion.

However, due to various practical reasons (such as data anonymization, 
incomplete data logs, etc.), the temporal ordering
(i.e. the time dimension) might be lost. Usually, in a time-series dataset we
would know the process states through a sequence of time steps. But when this ``hidden" time
dimension is lost, we face another challenge of sorting the data in the correct temporal order
before using it to infer the underlying SDE's parameters.

Our goal is to correctly estimate the parameters $\mathbf{A}$ (drift), and
$\mathbf{H} = \mathbf{GG}^{\text{T}}$ (observational diffusion) when given only the
observational data. Furthermore, building on the recent identifiability theory of Wang et al., 2023
\cite{NEURIPS2023_ca642f8e}, who show that for linear SDEs driven by additive Brownian noise the
infinitesimal generator are generically identifiable from i.i.d. trajectories, provided
a simple full-rank moment condition:
\begin{equation}
\text{rank}\bigl[x_0, \mathbf{A}x_0, ..., \mathbf{A}^{d-1}x_0,
\mathbf{GG}^T, \mathbf{AGG}^T, ..., \mathbf{A}^{d-1}\mathbf{GG}^T
\bigr] = d
\end{equation}\label{first_eqt}
is satisfied.

Their result guarantees that, once the condition is met, every post-intervention distribution in the
sense of stochastic structural causal models is determined by the observational data.

\section{Proposed Method}\label{s:2}
We propose our base method with an extension based on Wang et al., \cite{NEURIPS2023_ca642f8e}.

\subsection{Iterative Two-Step Scheme} \mbox{}\\
First, we randomly initialize the parameters
$\mathbf{A}^{(0)}$ and $\mathbf{H}^{(0)} (= \mathbf{GG}^{\text{T}})$. Then, we follow a two-step
iterative scheme as follows:
\begin{enumerate}
  \item[(a)] \textbf{Update Previously-Sorted Segments} \\
    Keeping $\mathbf{A}^{(n)}$, and $\mathbf{H}^{(n)}$ at iteration $n$ fixed, rearrange the
    previously sorted segments for each trajectory to maximize the log-likelihood:
    \[
    \{\widetilde{\mathbf{X}}^{(m)}\}
    = 
    \arg\max_{\{\widetilde{\mathbf{X}}^{(m)}\}}
    \sum_{m=1}^M
    \ln p\bigl(\widetilde{\mathbf{X}}^{(m)} \mid \mathbf{A}^{(n)}, \mathbf{H}^{(n)}\bigr).
    \]
  \item[(b)] \textbf{Update SDE Parameters} \\
    With the newly completed trajectories fixed, we can update the SDE parameters also by maximum likelihood estimation:
    \[
    \mathbf{A}^{(k)}
    = 
    \frac{1}{\Delta t}
    \Bigl(\sum_{i, j}(\Delta\mathbf{X}_i^{(j)})\mathbf{X}_i^{(j)\text{T}}\Bigr)
    \Bigl(\sum_{i, j}\mathbf{X}_i^{(j)}\mathbf{X}_i^{(j)\text{T}}\Bigr)^{-1},
    \]
    \[
    \mathbf{H}^{(k)} 
    = 
    \frac{1}{T}
    \sum_{i, j}
    \bigl(\Delta\mathbf{X}_i^{(j)} - \mathbf{A}^{(k)}\mathbf{X}_i^{(j)}\Delta t\bigr)
    \bigl(\Delta\mathbf{X}_i^{(j)} - \mathbf{A}^{(k)}\mathbf{X}_i^{(j)}\Delta t\bigr)^{\text{T}},
    \]
    where $T = (N - 1)\Delta t$ is the total time and 
    $\Delta\mathbf{X}_i^{(j)} = \mathbf{X}_{i+1}^{(j)} - \mathbf{X}_i^{(j)}$.
  \end{enumerate}

\subsection{Extension to Generator Identification} \mbox{}\\
After we arrive at our estimated parameters, we can use the test (\ref{first_eqt}) to check for generator identifiability.
Namely, we test if:
\[
\text{rank}\bigl[\hat{x}_0, \hat{\mathbf{A}}\hat{x}_0, ..., \hat{\mathbf{A}}^{d-1}\hat{x}_0,
\widehat{\mathbf{GG}^T}, \hat{\mathbf{A}}\widehat{\mathbf{GG}^T}, ..., \hat{\mathbf{A}}^{d-1}\widehat{\mathbf{GG}^T}
\bigr] = d
\]
If our estimated params and re-ordered data pass this test, we can leverage the results in \cite{NEURIPS2023_ca642f8e} to
move forward with the SDE's generator ddentification.
\section{Notes}\label{s:3}
Some notes to keep in mind:
\begin{itemize}
  \item Proving convergence of the iterative scheme.
  \item Leveraging the identifiability conditions presented in \cite{guan2024identifyingdriftdiffusioncausal}.
\end{itemize}


\bibliographystyle{NAplain}
\bibliography{main}

\end{sloppypar}
\end{document}