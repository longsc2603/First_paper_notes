\documentclass[a4paper, 11pt]{article}
\hyphenpenalty=8000
\textwidth=125mm
\textheight=185mm

\usepackage{graphicx}
%this package is flexible for image insertion
%
\usepackage{alltt}
%this package is suitable for the description of algorithms and computer programs
%
\usepackage{amsmath}
%this package draws mathematical symbols smoothly
%
\usepackage[hidelinks, pdftex]{hyperref}
%this package produces hypertext links in the document

\pagenumbering{arabic}
\setcounter{page}{1}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\doi}[1]{\href{https://doi.org/#1}{\texttt{https://doi.org/#1}}}

\begin{document}
\begin{sloppypar}
\begin{center}
Applied Artificial Intelligence Institute (A2I2)\\
Deakin University\\
\today\\[24pt]
\LARGE
\textbf{Problem Notes}\\[6pt]
\small
\textbf {Long Van Tran}\\[6pt]
s224930257@deakin.edu.au\\[6pt]
% Received: date\quad/\quad
% Revised: date\quad/\quad
% Publised online: data
\end{center}

% \begin{abstract}
% A short abstract describing the research done, methodology, and achieved results is to be presented. The abstract should contain approximately 100 words. The volume of the article is up to 20 pages in NA journal style. The journal recognizes review articles as special ones. These articles (if any) will occupy the starting positions in the journal and may contain more than 20 pages. Text in article have to follow a few simple guidelines: complex mathematical expressions have to be justified (like in the excerpt below), algorithms have to be presented in the style of \texttt{alltt}, \textit{postscript specials} have to be absent in file format presenting images. The enumeration of references within any article has to be organized in alphabetic order (see the sample below). References have to be distinguished between journal articles, collective works, and books and presented in BibTeX\ \texttt{NAplain} style (see the corresponding section below and the file \texttt{sample.bib}). \vskip 2mm

% \textbf{Keywords:} a few keywords (2--5) essential to the content of the article.

% \end{abstract}


\section{Problem Statement}\label{s:1}
In a conventional setting, we have a time-series dataset $\{\mathbf{X}^{(m)}\}_{m=1}^M$,
\[
\mathbf{X}^{(m)} 
= 
\bigl[\mathbf{X}_{1}^{(m)}, \mathbf{X}_{2}^{(m)}, \ldots, \mathbf{X}_{T}^{(m)}\bigr],
\]
where $\mathbf{X}_{i}^{(m)}$ can be vectors ($\mathbf{X}_{i}^{(m)} \in \mathbf{R}^d$) or matrices
($\mathbf{X}_{i}^{(m)} \in \mathbf{R}^{t \times d}$), depending on the unit of time being considered
here is a single time step or a period of $t$ steps. Here we consider the latter matrix case.
Each of these trajectories describe patient records across \(T\) time periods
(e.g., patient measurements throughout \(T\) hours).

However, due to various practical reasons (such as data anonymization, 
incomplete data logs, etc.), the temporal ordering (i.e. the time dimension) is lost.
Furthermore, the trajectory data is also missing, with the assumption that for each of
T time period, there is always at least one trajectory with data present. Thus, instead
of a temporally-labeled complete dataset $\{\mathbf{X}^{(m)}\}_{m=1}^M$,
we have a dataset of M temporally-unordered partially-completed trajectories.

We assume that the data is governed by a time-homogeneous linear additive noise stochastic
differential equation (SDE), which has the form:
\[
\mathrm{d}\mathbf{X}_t 
= 
\mathbf{A}\mathbf{X}_t\mathrm{d}t 
\;+\; 
\mathbf{G}\mathrm{d}\mathbf{W}_t,
\]
with unknown drift-diffusion parameters $\mathbf{A} \in \mathbf{R}^{d \times d}$ and
$\mathbf{G} \in \mathbf{R}^{d \times m}$. $\mathbf{W}$ is an $m$-dimensional Brownian motion.
Our aim is to:
\begin{enumerate}
\item Reconstruct missing segments into full (or partially completed) trajectories
$\{\widetilde{\mathbf{X}}^{(m)}\}$. This aim is intermediate.
\item Estimate $\mathbf{A}$ and $\mathbf{G}$ by maximum likelihood, while enforcing a Directed
Acyclic Graph (DAG) constraint on the global ordering of the reconstructed segments
(thus avoiding ``cyclic'' order). This is our final and most important aim.
\end{enumerate}

\section{Proposed Method}\label{s:2}
We proposed a solution that works as follows:

\paragraph{2.1. Sorting Data by Variance}
\begin{itemize}
  \item Empirically evaluate the variance of each observed segment of all
  our trajectories.
  \item Sort segments in ascending order of variance, with the assumption that we
  are dealing with a diverging SDE and that a diverging SDE usually has increasing
  variance over (hidden) time. This can be done in a similar manner for converging SDEs.
\end{itemize}

\paragraph{2.2. Iterative Two-Step Scheme} \mbox{}\\
First, we randomly initialize the parameters
$\mathbf{A}^{(0)}$ and $\mathbf{G}^{(0)}$. Then, we follow a two-step
iterative scheme as follows:
\begin{enumerate}
  \item[(a)] \textbf{Update Previously-Sorted Segments} \\
    Keeping $\mathbf{A}^{(n)}$ and $\mathbf{G}^{(n)}$ at iteration $n$ fixed, rearrange the
    previously sorted segments for each trajectory to maximize the log-likelihood minus a
    DAG penalty $\Omega_{\mathrm{DAG}}$:
    \[
    \{\widetilde{\mathbf{X}}^{(m)}\}
    = 
    \arg\max_{\{\widetilde{\mathbf{X}}^{(m)}\}}
    \sum_{m=1}^M
    \ln p\bigl(\widetilde{\mathbf{X}}^{(m)} \mid \mathbf{A}^{(n)}, \mathbf{G}^{(n)}\bigr)
    \;-\; 
    \Omega_{\mathrm{DAG}}\bigl(\{\widetilde{\mathbf{X}}^{(m)}\}\bigr).
    \]
    A well-known continuous DAG-penalty comes from the NOTEARS approach by
    Zheng et al., 2018 \cite{zheng2018dagstearscontinuousoptimization}:
    \[
    h(\mathbf{A})
    = \text{trace}(e^{\mathbf{A} \circ \mathbf{A}})-d,
    \]
    \[
    \Omega_{\mathrm{DAG}}\bigl(\{\widetilde{\mathbf{X}}^{(m)}\}\bigr)
    = \alpha (h(\mathbf{A})),
    \]
    where $d$ is the number of variables (i.e. the size of the square matrix
    $\mathbf{A}$) and $\alpha$ is a regularization hyper-parameter.
    It is proved in \cite{zheng2018dagstearscontinuousoptimization} that
    $h(\mathbf{A}) = 0$ if and only if $\mathbf{A}$,
    our adjacency matrix, corresponds to a DAG (no directed cycles). If cycles exist,
    $h(\mathbf{A}) > 0$. Hence, adding this term as a penalty encourages $\mathbf{A}$
    to remain acyclic.
  \item[(b)] \textbf{Update SDE Parameters} \\
    With the newly completed trajectories fixed, re-estimate the SDE parameters:
    \[
    (\mathbf{A}^{(n+1)}, \mathbf{G}^{(n+1)}) 
    = 
    \arg\max_{\mathbf{A}, \mathbf{G}}
    \sum_{m=1}^M
    \ln p\bigl(\widetilde{\mathbf{X}}^{(m)} \mid \mathbf{A}^{(n)}, \mathbf{G}^{(n)}\bigr).
    \]
    This step can be done using the parameter estimation framework named APPEX introduced
    by Guan et al., 2024 \cite{guan2024identifyingdriftdiffusioncausal}.
  \end{enumerate}

\section{Notes}\label{s:3}
Some notes to keep in mind:
\begin{itemize}
  \item Proving convergence of the iterative scheme.
  \item Leveraging the identifiability conditions presented in \cite{guan2024identifyingdriftdiffusioncausal}.
\end{itemize}


\bibliographystyle{NAplain}
\bibliography{main}

\end{sloppypar}
\end{document}