\documentclass[a4paper, 11pt]{article}
\hyphenpenalty=8000
\textwidth=125mm
\textheight=185mm

\usepackage{graphicx}
%this package is flexible for image insertion
%
\usepackage{alltt}
%this package is suitable for the description of algorithms and computer programs
%
\usepackage{amsmath}
%this package draws mathematical symbols smoothly
%
\usepackage[hidelinks, pdftex]{hyperref}
%this package produces hypertext links in the document

\pagenumbering{arabic}
\setcounter{page}{1}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\doi}[1]{\href{https://doi.org/#1}{\texttt{https://doi.org/#1}}}

\begin{document}
\begin{sloppypar}
\begin{center}
Applied Artificial Intelligence Institute (A2I2)\\
Deakin University\\
\today\\[24pt]
\LARGE
\textbf{Problem Overview}\\[6pt]
\small
\textbf {Long Van Tran}\\[6pt]
s224930257@deakin.edu.au\\[6pt]
% Received: date\quad/\quad
% Revised: date\quad/\quad
% Publised online: data
\end{center}

% \begin{abstract}
% A short abstract describing the research done, methodology, and achieved results is to be presented. The abstract should contain approximately 100 words. The volume of the article is up to 20 pages in NA journal style. The journal recognizes review articles as special ones. These articles (if any) will occupy the starting positions in the journal and may contain more than 20 pages. Text in article have to follow a few simple guidelines: complex mathematical expressions have to be justified (like in the excerpt below), algorithms have to be presented in the style of \texttt{alltt}, \textit{postscript specials} have to be absent in file format presenting images. The enumeration of references within any article has to be organized in alphabetic order (see the sample below). References have to be distinguished between journal articles, collective works, and books and presented in BibTeX\ \texttt{NAplain} style (see the corresponding section below and the file \texttt{sample.bib}). \vskip 2mm

% \textbf{Keywords:} a few keywords (2--5) essential to the content of the article.

% \end{abstract}


\section{Problem Setting}\label{s:1}
In a conventional setting, we have a time-series dataset $\{\mathbf{X}^{(m)}\}_{m=1}^M$,
\[
\mathbf{X}^{(m)} 
= 
\bigl[\mathbf{X}_{t_{1}}^{(m)}, \mathbf{X}_{t_{2}}^{(m)}, \ldots, \mathbf{X}_{t_{k}}^{(m)}\bigr],
\]
where $\mathbf{X}_{t_{i}}^{(m)}$ can be vectors ($\mathbf{X}_{t_{i}}^{(m)} \in \mathbf{R}^d$) or matrices
($\mathbf{X}_{t_{i}}^{(m)} \in \mathbf{R}^{t \times d}$), depending on the unit of time being considered
here is a single time step or a segment of $t$ steps. Here we consider the latter matrix case.
Each of these trajectories describe patient records across \(k\) time points/segments
(e.g., patient measurements throughout \(k\) hours).

We assume that the data is governed by a time-homogeneous linear additive noise stochastic
differential equation (SDE), which has the form:
\[
\mathrm{d}\mathbf{X}_t 
= 
\mathbf{A}\mathbf{X}_t\mathrm{d}t 
\;+\; 
\mathbf{G}\mathrm{d}\mathbf{W}_t,
\]
with unknown drift-diffusion parameters $\mathbf{A} \in \mathbf{R}^{d \times d}$ and
$\mathbf{G} \in \mathbf{R}^{d \times m}$. $\mathbf{W}$ is an $m$-dimensional Brownian motion.

However, due to various practical reasons (such as data anonymization, 
incomplete data logs, etc.), the temporal ordering
(i.e. the time dimension) might be lost or the time values 
at which measurements are taken might be noisy (measurement errors).
Thus, we want to find solutions for these complex real-world challenges.

\subsection{Data without Temporal Order}
Our first data complication is measurements taken without any temporal order. Usually,
in a time-series dataset we would know the process states through a sequence of time steps.
But when this "hidden" time dimension is lost, we face another challenge of sorting the data
in the correct temporal order before using it to infer the underlying SDE's parameters.

\subsection{Noisy Data Measurements}
Other than missing temporal order completely, another complication that can happen to
our data is noisy measurements, whereas our data is either measured incorrectly,
or the time at which measurements are taken are logged incorrectly. In other words,
either the data values $\mathbf{X}_{t_{i}}^{(m)}$ or their corresponding time points
$t_{i}$ are presented with independent,
randomly distributed noise.

For both these data settings, our aim is to estimate the parameters $\mathbf{A}$ and
$\mathbf{G}$ while enforcing a Directed Acyclic Graph (DAG) constraint on the
adjacency matrix $\mathbf{A}$ (thus avoiding ``cyclic'' order).

\section{Proposed Method}\label{s:2}
For each of the two mentioned data complications, we proposed a solution outlined as follow:

\subsection{Data without Temporal Order}
This solution consists of two parts, whereas part two is a two-step iterative scheme where
we estimate the parameters of the SDE.
\subsubsection{Sorting Data by Variance}
\begin{itemize}
  \item Empirically evaluate the variance of each observed segment of all
  our trajectories.
  \item Sort segments in ascending order of variance, with the assumption that we
  are dealing with a diverging SDE and that a diverging SDE usually has increasing
  variance over (hidden) time. This can be done in a similar manner for converging SDEs.
\end{itemize}

\subsubsection{Iterative Two-Step Scheme} \mbox{}\\
First, we randomly initialize the parameters
$\mathbf{A}^{(0)}$ and $\mathbf{G}^{(0)}$. Then, we follow a two-step
iterative scheme as follows:
\begin{enumerate}
  \item[(a)] \textbf{Update Previously-Sorted Segments} \\
    Keeping $\mathbf{A}^{(n)}$ and $\mathbf{G}^{(n)}$ at iteration $n$ fixed, rearrange the
    previously sorted segments for each trajectory to maximize the log-likelihood minus a
    DAG penalty $\Omega_{\mathrm{DAG}}$:
    \[
    \{\widetilde{\mathbf{X}}^{(m)}\}
    = 
    \arg\max_{\{\widetilde{\mathbf{X}}^{(m)}\}}
    \sum_{m=1}^M
    \ln p\bigl(\widetilde{\mathbf{X}}^{(m)} \mid \mathbf{A}^{(n)}, \mathbf{G}^{(n)}\bigr)
    \;-\; 
    \Omega_{\mathrm{DAG}}\bigl(\{\widetilde{\mathbf{X}}^{(m)}\}\bigr).
    \]
    A well-known continuous DAG-penalty comes from the NOTEARS approach by
    Zheng et al., 2018 \cite{zheng2018dagstearscontinuousoptimization}:
    \[
    h(\mathbf{A})
    = \text{trace}(e^{\mathbf{A} \circ \mathbf{A}})-d,
    \]
    \[
    \Omega_{\mathrm{DAG}}\bigl(\{\widetilde{\mathbf{X}}^{(m)}\}\bigr)
    = \alpha (h(\mathbf{A})),
    \]
    where $d$ is the number of variables (i.e. the size of the square matrix
    $\mathbf{A}$) and $\alpha$ is a regularization hyper-parameter.
    It is proved in \cite{zheng2018dagstearscontinuousoptimization} that
    $h(\mathbf{A}) = 0$ if and only if $\mathbf{A}$,
    our adjacency matrix, corresponds to a DAG (no directed cycles). If cycles exist,
    $h(\mathbf{A}) > 0$. Hence, adding this term as a penalty encourages $\mathbf{A}$
    to remain acyclic.
  \item[(b)] \textbf{Update SDE Parameters} \\
    With the newly completed trajectories fixed, re-estimate the SDE parameters:
    \[
    (\mathbf{A}^{(n+1)}, \mathbf{G}^{(n+1)}) 
    = 
    \arg\max_{\mathbf{A}, \mathbf{G}}
    \sum_{m=1}^M
    \ln p\bigl(\widetilde{\mathbf{X}}^{(m)} \mid \mathbf{A}^{(n)}, \mathbf{G}^{(n)}\bigr).
    \]
    This step can be done using the parameter estimation framework named APPEX introduced
    by Guan et al., 2024 \cite{guan2024identifyingdriftdiffusioncausal}.
  \end{enumerate}

\subsection{Noisy Data Measurements}
Based on the theory of Gaussian Process and the work of Mchutchon et al. \cite{NIPS2011_a8e864d0},
we propose using the noisy input data/time steps to directly learn the underlying SDE's parameters.

\subsubsection{Using a Gaussian Process Surrogate for the Drift}
We want to estimate the drift function $f(x)$ of the SDE (in this case the matrix $\mathbf{A}$).
But because we only observe noisy inputs $Y_t = X_t + \epsilon_t$ with $\epsilon_t \sim \mathcal{N}(0, \Sigma_x)$,
fitting a standard GP to the pairs $(Y_t, Y'_t)$ would lead to biased estimates due to the input
noise. This uncertainty in $Y_t$ affects the estimate of $f(x)$ non-uniformly — especially in regions
where $f(x)$ changes rapidly.

To account for this, we:
\begin{itemize}
  \item Treat the Euler–Maruyama-discretized form as a supervised learning problem, whereas:
        \[
          Y_{t+1} \approx Y_t + AY_tdt + \text{noise}
        \]
  \item Fit a GP to the pairs $\{(Y_t, Y_{t+1})\}$. This GP will provide a posterior mean function
        $\bar{f}(x) \approx \mathbf{A}x$, which allows us to compute the gradient $\nabla \bar{f}(x)$
        analytically. Note that in GPs, derivatives are also GPs and easy to compute.
\end{itemize}

\subsubsection{Applying Noisy-Input GP (NIGP) Correction}
NIGP use the first-order Taylor expansion of $f(x + \epsilon_x)$ around the mean input $x$:
\[
  f(x + \epsilon_x) \approx f(x) + \nabla\bar{f}(x)^T \epsilon_x
\]
Then, the variance in the output due to input noise is approximately:
\[
  \text{Var}[x + \epsilon_x] \approx \nabla\bar{f}(x)^T \Sigma_x \nabla\bar{f}(x) \approx \nabla\bar{f}(y)^T \Sigma_x \nabla\bar{f}(y)
\]
Thus, the GP posterior is corrected with this heteroscedastic variance term instead of assuming
constant noise variance $\Sigma_y$:
\[
  \text{Var}(y) = \Sigma_y + \nabla\bar{f}(y)^T \Sigma_x \nabla\bar{f}(y)
\]
This gives us a modified GP model that model input noise as heteroscedastic output noise — larger
variance in high-gradient regions, tighter confidence in flatter regions.

\subsubsection{Constructing Marginal Likelihood}
The negative log likelihood (NLL) of the dataset under the GP with this structured noise becomes:
\[
  \text{log}p(Y_{t+1}|Y_t) = \frac{1}{2}\sum_{t} [(Y_{t+1}-\phi Y_t)^T(\Sigma_y+\phi^T\Sigma_x\phi)^{-1}(Y_{t+1}-\phi Y_t)+\text{log}|\Sigma_y+\phi^T\Sigma_x\phi|],
\]
where $\Sigma_x$ is the input noise covariance to be inferred. Maximizing the above NLL with respect to:
\begin{itemize}
  \item $\phi = I + Adt$ gives $\mathbf{A}$
  \item $\Sigma_y = GG^Tdt$ gives $\mathbf{G}$
\end{itemize}
Our iterative procedure should be:
\begin{enumerate}
  \item Initialize $\mathbf{A}, \mathbf{A}, \Sigma_x$
  \item Use GP with the input-dependent noise $\text{Var}(Y)$ above
  \item Minimize the NLL with respect to parameters via gradients (using L-BFGS, Adam, ...)
  \item Re-estimate gradients based on updated GP predictions and iterate
\end{enumerate}
\section{Notes}\label{s:3}
Some notes to keep in mind:
\begin{itemize}
  \item Proving convergence of the iterative scheme.
  \item Leveraging the identifiability conditions presented in \cite{guan2024identifyingdriftdiffusioncausal}.
\end{itemize}


\bibliographystyle{NAplain}
\bibliography{main}

\end{sloppypar}
\end{document}